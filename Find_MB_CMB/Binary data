import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS
from jax import random, vmap
import numpy
from itertools import combinations
import operator
import pandas as pd
import arviz as az


rng_key = random.PRNGKey(0)
rng_key, rng_key_ = random.split(rng_key)

numpyro.set_host_device_count(2)
def Generate_Observational_Data(sample_size):

    #beta_X > Z1*X +Z2*X
    beta_X = jnp.array([1.3, 1.5])
    #beta_Y > X*Y + Z2*Y
    beta_Y = jnp.array([1.4, 1.15])

    Z1 = 0 + 15 * random.normal(random.PRNGKey((numpy.random.randint(1000))), (sample_size, 1))
    Z2 = 0 + 10 * random.normal(random.PRNGKey((numpy.random.randint(1000))), (sample_size, 1))
    data_Z1_Z2 = jnp.concatenate((Z1, Z2), axis=1)

    logits_X = jnp.sum(beta_X * data_Z1_Z2, axis=-1)
    X = dist.Bernoulli(logits=logits_X).sample(random.PRNGKey((numpy.random.randint(1000))))
    X = X.reshape(-1, 1)
    data_X_Z2 = jnp.concatenate((X, Z2), axis=1)
    logits_Y = jnp.sum(beta_Y * data_X_Z2, axis=-1)
    Y = dist.Bernoulli(logits=logits_Y).sample(random.PRNGKey((numpy.random.randint(1000))))

    labels = Y
    # data pane X,Z1,Z2
    data = jnp.concatenate((X, Z1, Z2), axis=1)

    return data, labels

def Generate_Experimental_Data(sample_size):

    #beta_Y > X*Y + Z2*Y
    beta_Y = jnp.array([1.4, 1.15])

    Z1 = 0 + 15 * random.normal(random.PRNGKey((numpy.random.randint(1000))), (sample_size, 1))
    Z2 = 0 + 10 * random.normal(random.PRNGKey((numpy.random.randint(1000))), (sample_size, 1))

    X = dist.Bernoulli(probs=0.5).sample(random.PRNGKey(numpy.random.randint(1000)), (sample_size, 1))
    data_X_Z2 = jnp.concatenate((X, Z2), axis=1)
    logits_Y = jnp.sum(beta_Y * data_X_Z2, axis=-1)
    Y = dist.Bernoulli(logits=logits_Y).sample(random.PRNGKey((numpy.random.randint(1000))))

    labels = Y
    # data pane X,Z1,Z2
    data = jnp.concatenate((X, Z1, Z2), axis=1)

    return data, labels

# Define the logistic regression model
def binary_logistic_regression(data, labels):

    D = data.shape[1]
    alpha = numpyro.sample("alpha", dist.Normal(0, 10))
    beta = numpyro.sample("beta", dist.Normal(jnp.zeros(D), 10 * jnp.ones(D)))
    logits = alpha + jnp.dot(data, beta)
    return numpyro.sample("obs", dist.Bernoulli(logits=logits), obs=labels)


# Define a function to compute the log likelihood
def log_likelihood_calculation(alpha, beta, data, obs):
    logits = alpha + jnp.dot(data, beta)
    log_likelihood = dist.Bernoulli(logits=logits).log_prob(obs)
    return log_likelihood.sum()

def sample_prior(data):

    prior_samples = {}
    D = data.shape[1]
    #paizei kai i paralagi me Cauchy(0, 2.5) gia tous coefs kai Cauchy(0, 10 ) gia to intercept
    prior_samples1 = dist.Normal(jnp.zeros(D), 10*jnp.ones(D)).sample(random.PRNGKey(0), (num_samples,))
    prior_samples2 = dist.Normal(jnp.zeros(1), 10*jnp.ones(1)).sample(random.PRNGKey(0), (num_samples,))

    prior_samples["beta"] = prior_samples1
    prior_samples["alpha"] = prior_samples2

    return prior_samples

# Perform MCMC with NUTS to sample from the posterior
def sample_posterior(data, observed_data):

    D = data.shape[1]

    kernel = NUTS(binary_logistic_regression)
    mcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)
    mcmc.run(jax.random.PRNGKey(1244), data, observed_data)

    # inf_data = az.from_numpyro(mcmc)
    # az.summary(inf_data)
    # corner.corner(inf_data, var_names=["alpha", "beta"])
    # plt.show()

    # Get the posterior samples
    posterior_samples = mcmc.get_samples()

    data_plot = az.from_numpyro(mcmc)
    az.plot_trace(data_plot, compact=True, figsize=(15, 25))
    # plt.show()

    # print(posterior_samples['alpha'])
    # fb_trace = 0
    # for i in range(D):
    #     fb_trace = (fb_trace + dist.Normal(numpy.mean(posterior_samples['beta'][:, i]),
    #                 numpy.std(posterior_samples['beta'][:, i])).
    #                 log_prob(posterior_samples['beta'][:, i]).sum())
    #
    #
    # fa_trace = (dist.Normal(numpy.mean(posterior_samples['alpha']), numpy.std(posterior_samples['alpha']))
    #             .log_prob(posterior_samples['alpha']).sum())
    #
    # pdf_posterior = (fa_trace + fb_trace)/num_samples

    return posterior_samples

# Calculate log likelihood for each posterior sample
def calculate_log_marginal(num_samples, samples, data, observed_data):
    log_likelihoods = jnp.zeros(num_samples)

    for i in range(num_samples):
        log_likelihoods = log_likelihoods.at[i].set(log_likelihood_calculation(samples["alpha"][i], samples["beta"][i],
                                                                               data, observed_data))

    # Estimate the log marginal likelihood using the log-sum-exp trick
    log_marginal_likelihood = jax.scipy.special.logsumexp(log_likelihoods) - jnp.log(num_samples)
    return log_marginal_likelihood

def var_combinations(data):
    #how many variables we have
    num_variables = data.shape[1]
    column_list = list(map(lambda var: var, range(0, num_variables)))

    df = pd.DataFrame(data, columns=column_list)
    sample_list = df.columns.values[0:]
    list_comb = []
    for l in range(df.shape[1]):
        list_combinations = list(combinations(sample_list, df.shape[1]-l))
        for x in list_combinations:
            if x[0] == 0:
                list_comb.append(x)
    return list_comb


correct_MB = 0
correct_IMB = 0

for i in range(1):

    data, observed_data = Generate_Observational_Data(1000)
    num_samples = 1000
    MB_Scores = {}
    IMB_Scores_obs = {}
    IMB_Scores_exp = {}

    list_comb = var_combinations(data)
    print(list_comb)

    for comb in range(len(list_comb)):
        reg_variables = list_comb[comb]

        sub_data = data[:, reg_variables]

        prior_samples = sample_prior(sub_data)

        marginal = calculate_log_marginal(num_samples, prior_samples, sub_data, observed_data)

        MB_Scores[reg_variables] = marginal


    MB_Do = max(MB_Scores.items(), key=operator.itemgetter(1))[0]
    print(MB_Scores)
    print(MB_Do)
    if MB_Do == (0, 2):
        correct_MB = correct_MB + 1

    sample_list = list(MB_Do)

    """Searching for subsets of MB"""
    subset_list = []
    for s in range(len(sample_list)):
        list_combinations = list(combinations(sample_list, len(sample_list) - s))
        for x in list_combinations:
            if x[0] == 0:
                subset_list.append(x)
    print('The subsets of MB are {}'.format(subset_list))

    exp_data, exp_observed_data = Generate_Experimental_Data(300)

    """For subsets of MB sample from experimental and observational data"""
    for j in range(len(subset_list)):
        reg_variables = subset_list[j]
        sub_data = data[:, reg_variables]
        exp_sub_data = exp_data[:, reg_variables]

        posterior_samples = sample_posterior(sub_data, observed_data)

        prior_samples = sample_prior(exp_sub_data)

        marginal = calculate_log_marginal(num_samples, prior_samples, exp_sub_data, exp_observed_data)
        print('Marginal {} from experimental sampling:'.format(reg_variables), marginal)
        IMB_Scores_exp[reg_variables] = marginal

        marginal = calculate_log_marginal(num_samples, posterior_samples, exp_sub_data, exp_observed_data)
        print('Marginal {} from observational sampling:'.format(reg_variables), marginal)

        IMB_Scores_obs[reg_variables] = marginal

    if IMB_Scores_obs[(0, 2)] > IMB_Scores_exp[(0, 2)] and IMB_Scores_obs[(0,)] < IMB_Scores_exp[(0,)]:
        correct_IMB = correct_IMB + 1

print(IMB_Scores_exp)
print(IMB_Scores_obs)
print(correct_MB)
print(correct_IMB)
