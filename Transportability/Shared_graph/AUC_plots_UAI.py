import matplotlib.pyplot as plt

from sklearn.metrics import roc_auc_score,  precision_score, f1_score
from sklearn import metrics


# For Ne = 50
true_labels = [0] * 20 + [1]*20
pred_prob = ([0.057374432663555464, 0.9417588639469753, 0.8833606397185245, 0.9037028046027845, 0.8665461871448654, 0.6228545920766132, 0.8217380722696117, 0.9794138387933963, 0.9878086236361574, 0.9772267136430539, 0.8062458160527288, 0.8344316578545451, 0.9911625577443305, 0.9930918974815377, 0.03670831160263144, 0.9991559851725687, 0.18349586953720354, 0.9348403937396107, 0.763829732791182, 0.9958628112086311]
             + [0.9967617688630923, 0.9989453838604522, 0.9993815987459821, 0.9981475085063157, 0.9995856445928637, 0.9879311975316141, 0.9899649269352849, 0.9978477926243821, 0.9974283786460286, 0.9987459694208866, 0.9459020834490476, 0.9989671413660259, 0.9937352380961549, 0.8321231776139995, 0.9988276030792544, 0.999529121959048, 0.9951993889944454, 0.9983993191703473, 0.9730171521220956, 0.9978620328650613])
fpr, tpr, thresholds = metrics.roc_curve(true_labels, pred_prob, pos_label=1)
auc_50_03 = metrics.auc(fpr, tpr)
print(f"AUC for Ne = 50: {auc_50_03}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 100
true_labels = [0] * 20 + [1] * 20
pred_prob = [0.9144084265652436, 0.947434646934957, 0.9940309524446248, 0.7655892466758262, 0.6193291924848907, 0.8639476684148135, 0.135585340257788, 0.16657793669909837, 0.9160198599456246, 0.967869411896927, 0.6826282821170631, 0.695680579872594, 0.5845435648886504, 0.6380374862656686, 0.9457660611088463, 0.942717202789142, 0.9890527795115716, 0.5167955127148337, 0.9904866487788486, 0.9949349411087648] + [1] * 20
fpr, tpr, thresholds = metrics.roc_curve(true_labels, pred_prob, pos_label=1)
auc_100_03 = metrics.auc(fpr, tpr)
print(f"AUC for Ne = 100: {auc_100_03}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 1000
true_labels = [0] * 20 + [1] *20
pred_prob = [5.198053051453432e-09, 2.3576653307398657e-11, 1.3489674022409795e-07, 8.248223224992388e-13, 2.998238656971791e-10, 2.323444160451983e-10, 3.9675515153146766e-11, 3.5228460602041336e-16, 2.3241208176945598e-15, 4.341778667287125e-16, 6.203495433583962e-12, 2.2857082237172358e-14, 1.4006872579391463e-06, 6.851826619624638e-14, 1.0811973099353181e-20, 1.0018824124756295e-15, 1.0829885256391066e-13, 4.1573940517218867e-07, 2.931883165487803e-16, 2.6486373648904828e-14] + [1] *20
auc_1000_03 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_1000_03}")

# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 50
true_labels = [0] * 20 + [1]*20
pred_prob = [0.576813, 0.353219, 0.392405, 0.019442, 0.924323, 0.160607, 0.924999, 0.460428, 0.511549, 0.001053, 0.968543, 0.386047, 0.669844, 0.000252, 0.846432, 0.000391, 0.765658, 0.022339, 0.194109, 0.000465] + [1]*20
auc_50_05 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 50: {auc_50_05}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 100
true_labels = [0] * 20 + [1] * 20
pred_prob = [9.639168e-02, 1.595581e-04, 2.677956e-02, 1.416842e-03, 4.027547e-04, 9.251237e-06, 4.332036e-05, 3.535109e-04, 1.241935e-04, 7.903761e-03, 2.842962e-03, 2.572610e-03, 9.624267e-03, 5.120148e-04, 5.211018e-04, 1.251170e-08, 2.764510e-02, 6.275978e-06, 1.941350e-07, 1.185106e-04] + [1] * 20
auc_100_05 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_100_05}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 1000
true_labels = [0] * 20 + [1] *20
pred_prob = [ 1.311653e-33, 2.353825e-44, 6.348697e-41, 1.491426e-41, 1.772280e-28, 1.352882e-62, 3.442305e-45, 3.587970e-55, 2.233186e-44, 2.633978e-43, 1.946475e-29, 6.245544e-43, 3.119803e-41, 2.701952e-42, 6.589605e-54, 2.360448e-54, 1.570023e-40, 2.645991e-44, 7.146850e-36, 7.726039e-40 ]  + [1] *20
auc_1000_05 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_1000_05}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 50
true_labels = [0] * 20 + [1]*20
pred_prob = [0.0001710715079200908, 0.6740634746905175, 0.045669000223265505, 0.6126003143073695, 0.0002183486215214223, 0.00027153051199384004, 0.41563709608925187, 0.004307949451780512, 1.7681666685499997e-05, 1.6462986086038246e-05, 0.020480479553899172, 0.889726804044525, 0.1377803914480945, 0.7226562471541531, 0.06882590689581297, 0.6373125604682379, 0.16445643554477424, 0.00013906364782450713, 0.5047682270270005, 0.10099216779651898] + [1]*20
auc_50_06 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 50: {auc_50_06}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 100
true_labels = [0] * 20 + [1] * 20
pred_prob = [1.218007794277102e-05, 1.0636178597501704e-07, 0.0005358091375473148, 2.8091734436713463e-07, 0.05049205945820854, 4.306482410418072e-06, 0.0002152956410197878, 0.008786215581892626, 2.760651212838908e-06, 3.8019349204133953e-07, 1.16248897683937e-05, 1.9093164492980183e-12, 0.005358848968709764, 0.03981375811060244, 2.392095433417861e-05, 1.604640308165541e-09, 0.00010649556451267337, 0.0020519930272883532, 0.0011524178417490261, 1.997488099162295e-08] + [1]*20
auc_100_06 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_100_06}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 1000
true_labels = [0] * 20 + [1] *20
pred_prob = [1.8925029731579041e-62, 4.0531275433889874e-78, 1.721711624190602e-69, 2.806530697251534e-73, 1.0115594630235789e-61, 4.18053185036083e-61, 1.0846712507600128e-58, 1.568084846016361e-54, 3.993934367604996e-68, 7.282721649026385e-74, 1.6709216076218975e-58, 1.559588708217858e-63, 7.329005581878085e-61, 5.556827491234135e-74, 2.093699363360759e-77, 2.4074332388732442e-71, 2.415677695983997e-62, 2.8490275622975992e-68, 3.937021162976986e-56, 2.399911379431184e-66] + [1]*20
auc_1000_06 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_1000_06}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 50
true_labels = [0] * 20 + [1]*20
pred_prob = [0.16635774240559964, 4.012073642578114e-05, 0.009190784150725398, 1.1561186095907081e-05, 2.7915438544181635e-10, 2.5823903232418448e-05, 0.002450698958848093, 0.12000407393733646, 1.7574547074052259e-06, 0.12812830066364958, 0.7719650658266615, 0.23363258315173938, 0.0004526814232590836, 0.00023533543730323337, 7.955658942935028e-06, 0.00015352023274317828, 0.03270009077964601, 0.05784372096701078, 0.14438620735148952, 6.464519970504546e-05] + [1]*20
auc_50_07 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 50: {auc_50_07}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 100
true_labels = [0] * 20 + [1] * 20
pred_prob = [4.293261892984773e-06, 1.6793574544149353e-12, 8.601715611820579e-07, 2.3887634723181757e-10, 4.575571485329653e-10, 5.465598387024616e-10, 1.2468812323763873e-06, 1.0542363394387283e-05, 5.172458450804894e-06, 0.00039028768073811075, 4.813324532204064e-07, 6.741011467570631e-09, 4.870001653496415e-10, 3.700285120274433e-09, 3.0608899176095836e-07, 5.815486444088456e-08, 2.7802986148840093e-11, 1.0889109280205093e-06, 0.0003846320283886192, 5.018746922838158e-11] + [1]*20
auc_100_07 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_100_07}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")

# For Ne = 1000
true_labels = [0] * 20 + [1] *20
pred_prob = [1.2399951349199133e-89, 3.206711008736088e-81, 3.626984278712359e-77, 6.125074278447482e-87, 7.702945691297898e-113, 3.5601605367064294e-83, 1.7574892794165604e-87, 2.458794420589593e-87, 1.553219267741444e-99, 1.2401373629876766e-96, 2.0898606862262498e-85, 2.414323452346272e-77, 1.4160537041224674e-112, 1.275131229991522e-76, 1.9265295990547564e-89, 1.2041861798853495e-87, 2.942730985934482e-90, 7.185117013479251e-82, 1.3544894079763186e-98, 4.26250411539011e-73] + [1]*20
auc_1000_07 = roc_auc_score(true_labels, pred_prob)
print(f"AUC for Ne = 100: {auc_1000_07}")
# Choose a threshold
threshold = 0.5
# Convert probabilities to binary predictions
predicted_labels = [1 if prob >= threshold else 0 for prob in pred_prob]
# Calculate Recall
recall =   f1_score(true_labels, predicted_labels)
print(f"Recall at threshold {threshold}: {recall:.2f}")


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score
import pandas as pd

y_true_50 = [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20# List of true labels for 20 datasets (N_e=50)
y_score_50 = (([0.576813, 0.353219, 0.392405, 0.019442, 0.924323, 0.160607, 0.924999, 0.460428, 0.511549, 0.001053, 0.968543, 0.386047, 0.669844, 0.000252, 0.846432, 0.000391, 0.765658, 0.022339, 0.194109, 0.000465,
             0.995576, 0.998349, 0.999562, 0.998166, 0.980897, 0.999642, 0.998767, 0.998827, 0.990275, 0.895641, 0.993120, 0.998332, 0.997952, 0.998256, 0.989167, 0.999509, 0.990718, 0.996615, 0.998493, 0.986399 ] + ([0.057374432663555464, 0.9417588639469753, 0.8833606397185245, 0.9037028046027845, 0.8665461871448654, 0.6228545920766132, 0.8217380722696117, 0.9794138387933963, 0.9878086236361574, 0.9772267136430539, 0.8062458160527288, 0.8344316578545451, 0.9911625577443305, 0.9930918974815377, 0.03670831160263144, 0.9991559851725687, 0.18349586953720354, 0.9348403937396107, 0.763829732791182, 0.9958628112086311]
             + [0.9967617688630923, 0.9989453838604522, 0.9993815987459821, 0.9981475085063157, 0.9995856445928637, 0.9879311975316141, 0.9899649269352849, 0.9978477926243821, 0.9974283786460286, 0.9987459694208866, 0.9459020834490476, 0.9989671413660259, 0.9937352380961549, 0.8321231776139995, 0.9988276030792544, 0.999529121959048, 0.9951993889944454, 0.9983993191703473, 0.9730171521220956, 0.9978620328650613])
              +  [0.0001710715079200908, 0.6740634746905175, 0.045669000223265505, 0.6126003143073695, 0.0002183486215214223, 0.00027153051199384004, 0.41563709608925187, 0.004307949451780512, 1.7681666685499997e-05, 1.6462986086038246e-05, 0.020480479553899172, 0.889726804044525, 0.1377803914480945, 0.7226562471541531, 0.06882590689581297, 0.6373125604682379, 0.16445643554477424, 0.00013906364782450713, 0.5047682270270005, 0.10099216779651898] + [1]*20)
              + [0.16635774240559964, 4.012073642578114e-05, 0.009190784150725398, 1.1561186095907081e-05, 2.7915438544181635e-10, 2.5823903232418448e-05, 0.002450698958848093, 0.12000407393733646, 1.7574547074052259e-06, 0.12812830066364958, 0.7719650658266615, 0.23363258315173938, 0.0004526814232590836, 0.00023533543730323337, 7.955658942935028e-06, 0.00015352023274317828, 0.03270009077964601, 0.05784372096701078, 0.14438620735148952, 6.464519970504546e-05] + [1]*20)

y_true_100 = [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20# List of true labels for 20 datasets (N_e=50)
y_score_100 = ((([9.639168e-02, 1.595581e-04, 2.677956e-02, 1.416842e-03, 4.027547e-04, 9.251237e-06, 4.332036e-05, 3.535109e-04, 1.241935e-04, 7.903761e-03, 2.842962e-03, 2.572610e-03, 9.624267e-03, 5.120148e-04, 5.211018e-04, 1.251170e-08, 2.764510e-02, 6.275978e-06, 1.941350e-07, 1.185106e-04,
             0.999284, 0.999513, 0.982438, 0.999588, 0.999518, 0.998413, 0.995620, 0.999516, 0.998880, 0.995266, 0.999821, 0.996482, 0.996488, 0.999799, 0.999244, 0.999668, 0.999485, 0.945974, 0.999316, 0.999645]
               + [0.9144084265652436, 0.947434646934957, 0.9940309524446248, 0.7655892466758262, 0.6193291924848907, 0.8639476684148135, 0.135585340257788, 0.16657793669909837, 0.9160198599456246, 0.967869411896927, 0.6826282821170631, 0.695680579872594, 0.5845435648886504, 0.6380374862656686, 0.9457660611088463, 0.942717202789142, 0.9890527795115716, 0.5167955127148337, 0.9904866487788486, 0.9949349411087648] + [1] * 20)
               + [1.218007794277102e-05, 1.0636178597501704e-07, 0.0005358091375473148, 2.8091734436713463e-07, 0.05049205945820854, 4.306482410418072e-06, 0.0002152956410197878, 0.008786215581892626, 2.760651212838908e-06, 3.8019349204133953e-07, 1.16248897683937e-05, 1.9093164492980183e-12, 0.005358848968709764, 0.03981375811060244, 2.392095433417861e-05, 1.604640308165541e-09, 0.00010649556451267337, 0.0020519930272883532, 0.0011524178417490261, 1.997488099162295e-08] + [1]*20)
               +  [4.293261892984773e-06, 1.6793574544149353e-12, 8.601715611820579e-07, 2.3887634723181757e-10, 4.575571485329653e-10, 5.465598387024616e-10, 1.2468812323763873e-06, 1.0542363394387283e-05, 5.172458450804894e-06, 0.00039028768073811075, 4.813324532204064e-07, 6.741011467570631e-09, 4.870001653496415e-10, 3.700285120274433e-09, 3.0608899176095836e-07, 5.815486444088456e-08, 2.7802986148840093e-11, 1.0889109280205093e-06, 0.0003846320283886192, 5.018746922838158e-11] + [1]*20)

y_true_1000 = [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20 + [0] * 20 + [1] * 20# List of true labels for 20 datasets (N_e=50)
y_score_1000 = ((([1.311653e-33, 2.353825e-44, 6.348697e-41, 1.491426e-41, 1.772280e-28, 1.352882e-62, 3.442305e-45, 3.587970e-55, 2.233186e-44, 2.633978e-43, 1.946475e-29, 6.245544e-43, 3.119803e-41, 2.701952e-42, 6.589605e-54, 2.360448e-54, 1.570023e-40, 2.645991e-44, 7.146850e-36, 7.726039e-40,
             0.999988, 1.000000, 0.999999, 0.999997, 0.999999, 1.000000, 0.999994, 1.000000, 1.000000, 0.999986, 0.999998, 0.999997, 0.999999, 0.999999, 0.999999, 0.999965, 0.999997, 0.999992, 0.999999, 1.000000]
                + [5.198053051453432e-09, 2.3576653307398657e-11, 1.3489674022409795e-07, 8.248223224992388e-13, 2.998238656971791e-10, 2.323444160451983e-10, 3.9675515153146766e-11, 3.5228460602041336e-16, 2.3241208176945598e-15, 4.341778667287125e-16, 6.203495433583962e-12, 2.2857082237172358e-14, 1.4006872579391463e-06, 6.851826619624638e-14, 1.0811973099353181e-20, 1.0018824124756295e-15, 1.0829885256391066e-13, 4.1573940517218867e-07, 2.931883165487803e-16, 2.6486373648904828e-14] + [1] *20)
                +  [1.8925029731579041e-62, 4.0531275433889874e-78, 1.721711624190602e-69, 2.806530697251534e-73, 1.0115594630235789e-61, 4.18053185036083e-61, 1.0846712507600128e-58, 1.568084846016361e-54, 3.993934367604996e-68, 7.282721649026385e-74, 1.6709216076218975e-58, 1.559588708217858e-63, 7.329005581878085e-61, 5.556827491234135e-74, 2.093699363360759e-77, 2.4074332388732442e-71, 2.415677695983997e-62, 2.8490275622975992e-68, 3.937021162976986e-56, 2.399911379431184e-66] + [1]*20)
                + [1.2399951349199133e-89, 3.206711008736088e-81, 3.626984278712359e-77, 6.125074278447482e-87, 7.702945691297898e-113, 3.5601605367064294e-83, 1.7574892794165604e-87, 2.458794420589593e-87, 1.553219267741444e-99, 1.2401373629876766e-96, 2.0898606862262498e-85, 2.414323452346272e-77, 1.4160537041224674e-112, 1.275131229991522e-76, 1.9265295990547564e-89, 1.2041861798853495e-87, 2.942730985934482e-90, 7.185117013479251e-82, 1.3544894079763186e-98, 4.26250411539011e-73] + [1]*20)


# Calculate AUC for each dataset
predicted_labels_50 = [1 if prob >= 0.5 else 0 for prob in y_score_50]
predicted_labels_100 = [1 if prob >= 0.5 else 0 for prob in y_score_100]
predicted_labels_1000 = [1 if prob >= 0.5 else 0 for prob in y_score_1000]
print()
print(predicted_labels_50)
auc_50 = f1_score(y_true_50, predicted_labels_50)
auc_100 = f1_score(y_true_100, predicted_labels_100)
auc_1000 = f1_score(y_true_1000, predicted_labels_1000)

# Create a DataFrame to combine all the data
data_50 = {'y_true': y_true_50, 'y_score': y_score_50, 'Ne': ["50"] * len(y_true_50)}
data_100 = {'y_true': y_true_100, 'y_score': y_score_100, 'Ne': ["100"] * len(y_true_100)}
data_1000 = {'y_true': y_true_1000, 'y_score': y_score_1000, 'Ne': ["1000"] * len(y_true_1000)}

df_50 = pd.DataFrame(data_50)
df_100 = pd.DataFrame(data_100)
df_1000 = pd.DataFrame(data_1000)

# Combine the datasets into one DataFrame
df_all = pd.concat([df_50, df_100, df_1000])

# Create a single figure for boxplots
plt.figure(figsize=(10, 6))

# Create boxplots for all datasets together
sns.boxplot(x="Ne", y="y_score", data=df_all, hue="y_true", palette="coolwarm", width=0.6)

# Title and AUC Information
plt.title(f"$N_{{o}}^*$=10000, (F1-Scores: $N_{{e_{{50}}}}$={auc_50:.2f}, $N_{{e_{{100}}}}$={auc_100:.2f}, $N_{{e_{{1000}}}}$={auc_1000:.2f})", fontsize=14)
plt.xlabel("$N_e$", fontsize=12)
plt.ylabel(f'P({r"$H_Z^t$ $|$ $D_e,D_o^*$"})', fontsize=12)

# Overlay AUC Information
plt.axhline(0.5, color="black", linestyle="--", label="Random Guess (0.5)")
plt.legend(title="True Class (1:= $H_Z^t$, 0:= $H_Z^{\overline{t}}$ )", fontsize=10)

# Display the plot
plt.tight_layout()
plt.show()

print(r"$N_{e_{50}}$")


import matplotlib.pyplot as plt

# Provided F1-Score Data
f1_scores = {
    50: [0.7, 0.83, 0.87, 0.98],   # F1-scores for Ne=50
    100: [0.69, 1, 1, 1],          # F1-scores for Ne=100
    1000: [1.00, 1, 1, 1],         # F1-scores for Ne=1000
}

# Prepare Data for Boxplot
Ne_values = list(f1_scores.keys())
data = [f1_scores[Ne] for Ne in Ne_values]

# Create the Boxplot
plt.figure(figsize=(8, 6))
plt.boxplot(data, labels=Ne_values, patch_artist=True,
            boxprops=dict(facecolor='lightblue', color='blue'),
            whis=[0, 100])  # Set whiskers to cover full data range


# Add Labels and Title
plt.xlabel('Ne Values', fontsize=12)
plt.ylabel('F1-Score', fontsize=12)
plt.title('Distribution of F1-Scores for Different Ne Values', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the Plot
plt.tight_layout()
plt.show()
